{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.enableHiveSupport().appName('AmazonReviewRcommender').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "Found 3 items\n",
      "-rw-r--r--   3 hongyiwderrick hongyiwderrick           0 2022-04-27 19:14 /user/hongyiwderrick/project/kcore_5.csv\n",
      "-rw-r--r--   3 hongyiwderrick hongyiwderrick 32072979001 2022-04-27 19:21 /user/hongyiwderrick/project/kcore_5.json\n",
      "-rw-r--r--   3 hongyiwderrick hongyiwderrick 10544467811 2022-04-27 19:29 /user/hongyiwderrick/project/metadata.json\n"
     ]
    }
   ],
   "source": [
    "#check to see if thata is available in HDFS\n",
    "!hdfs dfs -ls /user/hongyiwderrick/project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load kcore_5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata.json\n",
    "df_kcore = spark.read.json(\"/user/hongyiwderrick/project/kcore_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------------------+-----------+--------------+------------+--------------------+--------------+\n",
      "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|reviewerName|             summary|unixReviewTime|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+------------+--------------------+--------------+\n",
      "|0000013714| [0, 0]|    4.0|We use this type ...| 12 3, 2013| ACNGUPJ3A3TM9|         GCM|         Nice Hymnal|    1386028800|\n",
      "|0000013714| [2, 3]|    5.0|I bought this for...|09 13, 2009|A2SUAM1J3GNN3B| J. McDonald|Heavenly Highway ...|    1252800000|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+------------+--------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kcore.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kcore.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+----------+----------+----------+------------+-------+--------------+\n",
      "|asin|helpful|overall|reviewText|reviewTime|reviewerID|reviewerName|summary|unixReviewTime|\n",
      "+----+-------+-------+----------+----------+----------+------------+-------+--------------+\n",
      "|   0|      0|      0|         0|         0|         0|      932551|      0|             4|\n",
      "+----+-------+-------+----------+----------+----------+------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the number of missing values for each column\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df_kcore.select([count(when(df_kcore[c].isNull(), c)).alias(c) for c in df_kcore.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41135700"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kcore.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3035045"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kcore.select('reviewerID').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata.json\n",
    "df_meta = spark.read.json(\"/user/hongyiwderrick/project/metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9430088"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|_corrupt_record|      asin|brand|          categories|         description|               imUrl|price|             related|           salesRank|               title|\n",
      "+---------------+----------+-----+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|           null|0001048791| null|           [[Books]]|                null|http://ecx.images...| null|                null|[,,,,, 6334800,,,...|The Crucible: Per...|\n",
      "|           null|0000143561| null|[[Movies & TV, Mo...|3Pack DVD set - I...|http://g-ecx.imag...|12.99|[, [B0036FO6SI, B...|[,,,,,,,,,,,,,,,,...|Everyday Italian ...|\n",
      "+---------------+----------+-----+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meta.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge kcore with meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_meta.join(df_kcore, df_kcore.asin == df_meta.asin, \"inner\").select(\n",
    "    df_kcore.asin, \n",
    "    df_kcore.helpful,\n",
    "    df_kcore.overall, \n",
    "    df_kcore.reviewText, \n",
    "    df_kcore.reviewerID,\n",
    "    df_meta.categories,\n",
    "    df_meta.price,\n",
    "    df_meta.related,\n",
    "    df_meta.title\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------------------+-------------+----------+-----+--------------------+---------------+\n",
      "|      asin|helpful|overall|          reviewText|   reviewerID|categories|price|             related|          title|\n",
      "+----------+-------+-------+--------------------+-------------+----------+-----+--------------------+---------------+\n",
      "|0002216973| [1, 1]|    5.0|And to me, there'...|AESMLAZX4PI6L| [[Books]]| null|[, [0812823354, 0...|Red Adam's Lady|\n",
      "|0002216973| [1, 1]|    5.0|From the dust jac...|AMVV8VYDTLA78| [[Books]]| null|[, [0812823354, 0...|Red Adam's Lady|\n",
      "+----------+-------+-------+--------------------+-------------+----------+-----+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hive_context = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+\n",
      "|      database|           tableName|isTemporary|\n",
      "+--------------+--------------------+-----------+\n",
      "|hongyiwderrick|   business_licenses|      false|\n",
      "|hongyiwderrick|      chicago_crimes|      false|\n",
      "|hongyiwderrick|  chicago_crimes_new|      false|\n",
      "|hongyiwderrick|      facility_types|      false|\n",
      "|hongyiwderrick|     foodinspections|      false|\n",
      "|hongyiwderrick|foodinspections_p...|      false|\n",
      "|hongyiwderrick|             kcore_5|      false|\n",
      "+--------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive_context.sql(\"use hongyiwderrick\");\n",
    "hive_context.sql(\"show tables\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive_context.sql(\"drop table if exists kcore_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcore.write.mode('overwrite').saveAsTable('kcore_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|41135700|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive_context.sql(\"select count(*) from kcore_5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive_context.sql(\"use hongyiwderrick\");\n",
    "hive_context.sql(\"drop table if exists metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hive_context.sql(\"select count(*) from metadata\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.1.0-hadoop/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, lower\n",
    "from pyspark.sql.functions import udf\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# including stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# add some non-sense words\n",
    "stop_words += ['', \"-\", \"many\", '\"', \"go\", \"one\", \"much\", \"get\", \"also\", \"would\", \"make\", \"i'm\", \"i've\", \"us\", \"going\",\n",
    "               \"could\", \"made\", \"every\", \"another\", \"things\", \"may\", \"come\", \"know\", \"way\", \"see\", \"put\", \"say\", \"got\",\n",
    "               \"looking\", \"look\", \"give\", \"since\", \"next\", \"think\", \"thought\", \"bit\", \"little\", \"makes\", \"must\", \"lot\",\n",
    "               \"find\", \"done\", \"still\", \"everything\", \"might\", \"wanted\", \"&\", \")\", \"takes\", \"anything\", \"nothing\", \n",
    "               \"said\", \"that's\", \"5\", \"2\", \"gives\", \"--\", \"making\", \"given\", \"often\", \"comes\", \"getting\", \"set\", \n",
    "               \"able\", \"away\", \"felt\", \"using\", \"1\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"use\", \"feel\", \"used\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_num: list = df.groupBy(df.categories[0][0]).count().sort(\"count\", ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Books',\n",
       " 'Electronics',\n",
       " 'Clothing, Shoes & Jewelry',\n",
       " 'Movies & TV',\n",
       " 'Home & Kitchen',\n",
       " 'Health & Personal Care',\n",
       " 'Cell Phones & Accessories',\n",
       " 'Apps for Android',\n",
       " 'Sports & Outdoors',\n",
       " 'CDs & Vinyl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_categories_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    top_categories_list.append(categories_num[i][0])\n",
    "    \n",
    "top_categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top 10 categories \n",
    "\n",
    "df_review_cat = df.select(\"reviewText\",\"categories\").where(df.categories[0][0].isin(top_categories_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|          reviewText|categories|\n",
      "+--------------------+----------+\n",
      "|And to me, there'...| [[Books]]|\n",
      "|From the dust jac...| [[Books]]|\n",
      "|There are only a ...| [[Books]]|\n",
      "|I bought this boo...| [[Books]]|\n",
      "|I was looking out...| [[Books]]|\n",
      "|Lady Julitta de M...| [[Books]]|\n",
      "|I read this many ...| [[Books]]|\n",
      "|It seems cruel to...| [[Books]]|\n",
      "|I am thrilled to ...| [[Books]]|\n",
      "|Lady Julitta is m...| [[Books]]|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_review_cat.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compound_scores(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "   \n",
    "    # Get sentiment scores\n",
    "    compound_score = sia.polarity_scores(text)['compound']\n",
    "\n",
    "    return compound_score\n",
    "\n",
    "get_compound_scores_udf = udf(get_compound_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_scores(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "   \n",
    "    # Get sentiment scores\n",
    "    neg_score = sia.polarity_scores(text)['neg']\n",
    "\n",
    "    return neg_score\n",
    "\n",
    "get_neg_scores_udf = udf(get_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neu_scores(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "   \n",
    "    # Get sentiment scores\n",
    "    neu_score = sia.polarity_scores(text)['neu']\n",
    "\n",
    "    return neu_score\n",
    "\n",
    "get_neu_scores_udf = udf(get_neu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_scores(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "   \n",
    "    # Get sentiment scores\n",
    "    pos_score = sia.polarity_scores(text)['pos']\n",
    "\n",
    "    return pos_score\n",
    "\n",
    "get_pos_scores_udf = udf(get_pos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_cat = df_review_cat.withColumn(\"compound_score\", get_compound_scores_udf(F.col(\"reviewText\")))\n",
    "df_review_cat = df_review_cat.withColumn(\"neg_scores\", get_neg_scores_udf(F.col(\"reviewText\")))\n",
    "df_review_cat = df_review_cat.withColumn(\"neu_scores\", get_neu_scores_udf(F.col(\"reviewText\")))\n",
    "df_review_cat = df_review_cat.withColumn(\"pos_scores\", get_pos_scores_udf(F.col(\"reviewText\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------+----------+----------+----------+\n",
      "|          reviewText|categories|compound_score|neg_scores|neu_scores|pos_scores|\n",
      "+--------------------+----------+--------------+----------+----------+----------+\n",
      "|And to me, there'...| [[Books]]|        0.9708|     0.078|     0.742|      0.18|\n",
      "|From the dust jac...| [[Books]]|        0.6416|     0.072|     0.801|     0.126|\n",
      "+--------------------+----------+--------------+----------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_review_cat.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df_review_cat = df_review_cat.withColumn(\"compound_score\", df_review_cat[\"compound_score\"].cast(FloatType()))\n",
    "df_review_cat = df_review_cat.withColumn(\"neg_scores\", df_review_cat[\"neg_scores\"].cast(FloatType()))\n",
    "df_review_cat = df_review_cat.withColumn(\"neu_scores\", df_review_cat[\"neu_scores\"].cast(FloatType()))\n",
    "df_review_cat = df_review_cat.withColumn(\"pos_scores\", df_review_cat[\"pos_scores\"].cast(FloatType()))\n",
    "df_review_cat = df_review_cat.withColumn(\"categories\", df_review_cat[\"categories\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- compound_score: float (nullable = true)\n",
      " |-- neg_scores: float (nullable = true)\n",
      " |-- neu_scores: float (nullable = true)\n",
      " |-- pos_scores: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_review_cat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_cat_group = df_review_cat.groupBy(\"categories\").avg(\"compound_score\",\"neg_scores\",\"neu_scores\",\"pos_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_review_cat_group = df_review_cat.groupBy(\"categories\").agg(F.avg(\"compound_score\"),F.avg(\"neg_scores\"),F.avg(\"neu_scores\"),F.avg(\"pos_scores\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_cat_group1 = df_review_cat.groupBy(\"categories\").agg(F.avg(\"compound_score\"))\n",
    "df_review_cat_group2 = df_review_cat.groupBy(\"categories\").agg(F.avg(\"neg_scores\"))\n",
    "df_review_cat_group3 = df_review_cat.groupBy(\"categories\").agg(F.avg(\"neu_scores\"))\n",
    "df_review_cat_group4 = df_review_cat.groupBy(\"categories\").agg(F.avg(\"pos_scores\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- categories: string (nullable = true)\n",
      " |-- max(compound_score): float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_review_cat_group1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.functions' has no attribute 'percentile_approx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-1348744ba20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_review_cat_group1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_review_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile_approx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compound_score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'compound_score_med'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_review_cat_group2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_review_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile_approx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neg_scores\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neg_scores_med'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_review_cat_group3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_review_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile_approx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neu_scores\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neu_scores_med'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_review_cat_group4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_review_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile_approx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pos_scores\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos_scores_med'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.sql.functions' has no attribute 'percentile_approx'"
     ]
    }
   ],
   "source": [
    "#df_review_cat_group1 = df_review_cat.groupBy(\"categories\").agg(F.percentile_approx(\"compound_score\", 0.5).alias('compound_score_med'))\n",
    "#df_review_cat_group2 = df_review_cat.groupBy(\"categories\").agg(F.percentile_approx(\"neg_scores\", 0.5).alias('neg_scores_med'))\n",
    "#df_review_cat_group3 = df_review_cat.groupBy(\"categories\").agg(F.percentile_approx(\"neu_scores\", 0.5).alias('neu_scores_med'))\n",
    "#df_review_cat_group4 = df_review_cat.groupBy(\"categories\").agg(F.percentile_approx(\"pos_scores\", 0.5).alias('pos_scores_med'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_cat_group1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_cat_group1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sia_cat1 = df_review_cat_group1.join(df_review_cat_group2, df_review_cat_group1.categories == df_review_cat_group2.categories, \"inner\")\n",
    "#df_sia_cat2 = df_sia_cat1.join(df_review_cat_group3, df_review_cat_group3.categories == df_sia_cat1.categories, \"inner\")\n",
    "#df_sia_cat = df_sia_cat2.join(df_review_cat_group4, df_review_cat_group4.categories == df_sia_cat2.categories, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sia_cat_top15 = df_sia_cat.filter(df_sia_cat.categories.isin(top_categories_list[0:5]))\n",
    "#df_sia_cat_top610 = df_sia_cat.filter(df_sia_cat.categories.isin(top_categories_list[5:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_review_cat_group_top15.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf1 = df_review_cat_group1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf1 = df_review_cat_group.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1.plot(figsize=(20,7), kind='bar', x=\"categories\", grid=True, rot=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2.plot(figsize=(20,7), kind='bar', x=\"categories\", grid=True, rot=\"0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
